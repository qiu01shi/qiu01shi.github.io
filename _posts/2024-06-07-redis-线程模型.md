---
pin: true
layout: post
title: Linux IO 模型及 Redis IO 多路复用与事件派发的理解
author: 求是君
categories: [Blogging, Redis]
date: 2024-06-07 09:48 +0800
---

## 用户空间和内核态空间

任何 `Linux` 发行版，其系统内核都是 `Linux` 。我们的应用都需要通过 `Linux` 内核与硬件交互。



<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240705151406685.png" alt="image-20240705151406685" style="zoom:50%;" />



为了避免用户应用导致冲突甚至内核崩溃，用户应用与内核是分离的：

- 进程的寻址空间会划分为两部分：**内核空间**、**用户空间**；

- 用户空间只能执行受限的命令（`Ring3`），而且不能直接调用系统资源，必须通过内核提供的接口来访问；
- 内核空间可以执行特权命令（`Ring0`），调用一切系统资源；

比如一个 32 位的操作系统，他的带宽就是 32，他的虚拟地址就是  $2^{32}$  次方，也就是说他寻址的范围就是 0 ~ $2^{32}$ ， 这片寻址空间对应的就是 $2^{32}$ 个字节，就是 4GB，这个 4GB，会有 3 GB 分给用户空间，1GB 给内核系统；

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240607214058068.png" alt="image-20240607214058068" style="zoom:50%;" />

Linux系统为了提高 IO 效率，会在用户空间和内核空间都加入缓冲区：

- 写数据时，要把用户缓冲数据拷贝到内核缓冲区，然后写入设备；

- 读数据时，要从设备读取数据到内核缓冲区，然后拷贝到用户缓冲区；

针对这个操作：我们的用户在写读数据时，会去向内核态申请，想要读取内核的数据，而内核数据要去等待驱动程序从硬件上读取数据，当从磁盘上加载到数据之后，内核会将数据写入到内核的缓冲区中，然后再将数据拷贝到用户态的 `buffer` 中，然后再返回给应用程序，整体而言，速度慢，就是这个原因，为了加速，我们希望 `read` 也好，还是 `wait for data` 也最好都不要等待，或者时间尽量的短。

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240607212500821.png" alt="image-20240607212500821" style="zoom:55%;" />

## 5种 IO 模型

在《UNIX网络编程》一书中，总结归纳了 5 种 IO 模型：

- 阻塞IO（Blocking IO）
- 非阻塞IO（Nonblocking IO）
- IO多路复用（IO Multiplexing）
- 信号驱动IO（Signal Driven IO）
- 异步IO（Asynchronous IO）

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240607220602688.png" alt="image-20240607220602688" style="zoom:50%;" />

应用程序想要去读取数据，他是无法直接去读取磁盘数据的，他需要先到内核里边去等待内核操作硬件拿到数据，这个过程就是1，是需要等待的，等到内核从磁盘上把数据加载出来之后，再把这个数据写给用户的缓存区，这个过程是 2，如果是阻塞 IO，那么整个过程中，用户从发起读请求开始，一直到读取到数据，都是一个阻塞状态。

### 阻塞 IO

**阻塞 `IO`** 就是**两个阶段都必须阻塞等待：**

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240607221146281.png" alt="image-20240607221146281" style="zoom:50%;" />

**阶段一：**

- 用户进程尝试读取数据（比如网卡数据）
- 此时数据尚未到达，内核需要等待数据
- 此时用户进程也处于阻塞状态

**阶段二：**

* 数据到达并拷贝到内核缓冲区，代表已就绪
* 将内核数据拷贝到用户缓冲区
* 拷贝过程中，用户进程依然阻塞等待
* 拷贝完成，用户进程解除阻塞，处理数据

可以看到，阻塞 IO 模型中，用户进程在两个阶段都是阻塞状态。

### 非阻塞 IO

顾名思义，**非阻塞 `IO`** 的 `recvfrom` 操作会立即返回结果而不是阻塞用户进程。

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240607221904747.png" alt="image-20240607221904747" style="zoom:50%;" />

**阶段一：**

* 用户进程尝试读取数据（比如网卡数据）
* 此时数据尚未到达，内核需要等待数据
* 返回异常给用户进程
* 用户进程拿到 `error` 后，再次尝试读取
* 循环往复，直到数据就绪

**阶段二：**

* 将内核数据拷贝到用户缓冲区
* 拷贝过程中，用户进程依然阻塞等待
* 拷贝完成，用户进程解除阻塞，处理数据

可以看到，非阻塞 IO 模型中，用户进程在第一个阶段是非阻塞，第二个阶段是阻塞状态。虽然是非阻塞，但性能并没有得到提高。而且忙等机制会导致 `CPU` 空转，`CPU` 使用率暴增。

### IO 多路复用

无论是阻塞 IO 还是非阻塞 IO，用户应用在一阶段都需要调用 `recvfrom` 来获取数据，差别在于无数据时的处理方案：

- 如果调用 `recvfrom` 时，恰好没有数据，**阻塞 IO 会使 CPU 阻塞，非阻塞 IO 使 CPU 空转**，都不能充分发挥 CPU 的作用；
- 如果调用 `recvfrom` 时，恰好有数据，则用户进程可以直接进入第二阶段，读取并处理数据。

而在单线程情况下，**只能依次处理 IO 事件**，如果正在处理的 IO 事件恰好未就绪（数据不可读或不可写），线程就会被阻塞，所有IO 事件都必须等待，性能自然会很差。

就比如服务员给顾客点餐，**分两步**：

1. 顾客思考要吃什么（等待数据就绪）

2. 顾客想好了，开始点餐（读取数据）

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240607223204938.png" alt="image-20240607223204938" style="zoom:50%;" />

要提高效率有几种办法？

- 方案一：增加更多服务员（多线程）
- 方案二：不排队，谁想好了吃什么（数据就绪了），服务员就给谁点餐（用户应用就去读取数据）

那么问题来了：用户进程如何知道内核中数据是否就绪呢？



**文件描述符**（File Descriptor）：简称 `FD`，**是一个从 0 开始的无符号整数**，用来关联 `Linux` 中的一个文件。在 `Linux` 中，一切皆文件，例如常规文件、视频、硬件设备等，当然也包括网络套接字（`Socket`）。

**IO多路复用**：是利用 **单个线程来同时监听多个 `FD`**，并在某个 `FD` 可读、可写时得到通知，从而避免无效的等待，充分利用 `CPU`资源。

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240607223806039.png" alt="image-20240607223806039" style="zoom:50%;" />

**阶段一：**

* 用户进程调用 `select`，指定要监听的 `FD` 集合
* 内核监听 `FD` 对应的多个 `socket`
* 任意一个或多个 `socket` 数据就绪则返回 `readable`
* 此过程中用户进程阻塞

**阶段二：**

* 用户进程找到就绪的 `socket`
* 依次调用`recvfrom`读取数据
* 内核将数据拷贝到用户空间
* 用户进程处理数据

当用户去读取数据的时候，不再去直接调用 `recvfrom` 了，而是调用 `select` 的函数，`select` 函数会将需要监听的数据交给内核，由内核去检查这些数据是否就绪了，如果说这个数据就绪了，就会通知应用程序数据就绪，然后来读取数据，再从内核中把数据拷贝给用户态，完成数据处理，如果 `N` 多个 `FD` 一个都没处理完，此时就进行等待。

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240607224212738.png" alt="image-20240607224212738" style="zoom:50%;" />

**`IO` 多路复用** 是利用单个线程来同时监听多个 FD，并在某个 FD 可读、可写时得到通知，从而避免无效的等待，充分利用 `CPU` 资源。不过监听 `FD` 的方式、通知的方式又有多种实现，常见的有：

- select
- poll
- epoll

**差异：**

- `select` 和 `poll` 只会通知用户进程有 FD 就绪，但不确定具体是哪个 `FD`，需要用户进程 **逐个遍历 `FD` 来确认**；

- `epoll` 则会在通知用户进程 `FD` 就绪的同时，把已就绪的 `FD` 写入用户空间；



#### IO 多路复用-select

`select` 是 `Linux` 最早使用的 `I/O` 多路复用技术：



```c
// 定义类型别名 __fd_mask，本质是 long int  4 Bytes
typedef long int __fd_mask;

/* fd_set 记录要监听的fd集合，及其对应状态 */
typedef struct {
    // fds_bits 是 long 类型数组，长度为 1024/32 = 32
    // 共 1024 个bit位，每个 bit 位代表一个fd，0 代表未就绪，1 代表就绪
    __fd_mask fds_bits[__FD_SETSIZE / __NFDBITS];
    // ...
} fd_set;

// select函数，用于监听fd_set，也就是多个fd的集合
int select(
    int nfds,            // 要监视的fd_set的最大fd + 1
    fd_set *readfds,     // 要监听读事件的fd集合
    fd_set *writefds,    // 要监听写事件的fd集合
    fd_set *exceptfds,   // 要监听异常事件的fd集合
    struct timeval *timeout  // 超时时间，null-用不超时；0-不阻塞等待；大于0-固定等待时间
);
```



简单说，就是我们把需要处理的数据封装成 FD，然后在用户态时创建一个 fd 的集合（这个集合的大小是要监听的那个FD的最大值+1，但是大小整体是有限制的 ），**这个集合的长度大小是有限制的**，同时在这个集合中，标明出来我们要控制哪些数据，

比如要监听的数据，是 `1, 2, 5` 三个数据，此时会执行 `select` 函数，然后将整个 `fd` 发给内核态，内核态会去遍历用户态传递过来的数据，如果发现这里边都数据都没有就绪，就休眠，直到有数据准备好时，就会被唤醒，唤醒之后，再次遍历一遍，看看谁准备好了，然后再处理准备好的数据，最后再将这个 `FD` 集合写回到用户态中去，此时用户态就知道了，但是对于用户态而言，并不知道谁处理好了，所以用户态也需要去进行遍历，然后找到对应准备好数据的节点，再去发起读请求，我们会发现，这种模式下他虽然比阻塞 IO 和非阻塞 IO 好，但是依然有些麻烦的事情， 比如说 **频繁的传递 FD 集合，频繁的去遍历 FD 等问题。**

**用户空间拷贝到内核空间：**

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240705180417717.png" alt="image-20240705180417717" style="zoom:50%;" />

**内核空间拷贝到用户空间：**

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240705180612535.png" alt="image-20240705180612535" style="zoom:50%;" />

**select 模式存在的问题：**

- 需要将整个 `fd_set` 从用户空间拷贝到内核空间，`select` 结束还要再次拷贝回用户空间；
- `select` 无法得知具体是哪个 `fd` 就绪；
- 需要遍历整个 `fd_setfd_set` 监听的 `fd` 数量不能超过 `1024`。



#### IO多路复用-poll

`poll` 模式对 `select` 模式做了简单改进，但性能提升不明显，部分关键代码如下：

```c
// 定义 pollfd 中的事件类型
#define POLLIN   // 可读事件
#define POLLOUT  // 可写事件
#define POLLERR  // 错误事件
#define POLLNVAL // fd未打开

// pollfd 结构
struct pollfd {
    int fd;           // 要监听的fd
    short int events; // 要监听的事件类型：读、写、异常
    short int revents;// 实际发生的事件类型
};

// poll 函数
int poll(
    struct pollfd *fds, // pollfd 数组，可以自定义大小
    nfds_t nfds,        // 数组元素个数
    int timeout         // 超时时间
);
```

**IO流程：**

①：创建 `pollfd` 数组，向其中添加关注的 `fd` 信息，数组大小自定义；

②：调用 `poll` 函数，将 `pollfd` 数组拷贝到内核空间，转链表存储，无上限；

③：内核遍历 `fd`，判断是否就绪；

④：数据就绪或超时后，拷贝 `pollfd` 数组到用户空间，返回就绪 `fd` 数量 `n`；

⑤：用户进程判断 `n` 是否大于 `0`,大于 `0` 则遍历 `pollfd` 数组，找到就绪的 `fd`；

**与 `select` 对比：**

1. `select` 模式中的 `fd_set` 大小固定为 1024，而 `pollfd` 在内核中采用链表，理论上无上限；

2. 监听 `FD` 越多，每次遍历消耗时间也越久，性能反而会下降。



#### IO 多路复用-epoll

`epoll` 模式是对 `select` 和 `poll` 的改进，它提供了三个函数：

```c
struct eventpoll {
    //...
    struct rb_root rbr;        // 一颗红黑树，记录要监听的FD
    struct list_head rdlist;   // 一个链表，记录就绪的FD
    //...
};

// 1. 创建一个 epoll 实例, 内部是 event poll，返回对应的句柄 epfd
int epoll_create(int size);

// 2. 将一个 FD 添加到 epoll 的红黑树中，并设置 ep_poll_callback
//    callback 触发时，就把对应的 FD 加入到 rdlist 这个就绪列表中
int epoll_ctl(
    int epfd,                   // epoll 实例的句柄
    int op,                     // 要执行的操作，包括：ADD、MOD、DEL
    int fd,                     // 要监听的 FD
    struct epoll_event *event   // 要监听的事件类型：读、写、异常等
);

// 3. 检查 rdlist 列表是否为空，不为空则返回就绪的 FD 的数量
int epoll_wait(
    int epfd,                   // epoll 实例的句柄
    struct epoll_event *events, // 空 event 数组，用于接收就绪的 FD
    int maxevents,              // events 数组的最大长度
    int timeout                 // 超时时间，-1 永不超时；0 不阻塞；大于 0 为阻塞时间
);
```

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240705201152151.png" alt="image-20240705201152151" style="zoom:50%;" />



#### IO 多路复用总结

**select 模式存在的三个问题：**

- 能监听的 `FD` 最大不超过 `1024`
- 每次 `select` 都需要把所有要监听的 `FD` 都拷贝到内核空间
- 每次都要遍历所有 `FD` 来判断就绪状态

**poll 模式的问题：**

- `poll` 利用链表解决了 `select` 中监听 `FD` 上限的问题，但依然要遍历所有 `FD`，如果监听较多，性能会下降；

**epoll模式中如何解决这些问题的？**

- 基于 `epoll` 实例中的红黑树保存要监听的 `FD`，理论上无上限，而且增删改查效率都非常高每个 `FD` 只需要执行一次`epoll_ctl` 添加到红黑树，以后每次 `epol_wait` 无需传递任何参数，无需重复拷贝 `FD` 到内核空间；
- 利用 `ep_poll_callback` 机制来监听 `FD` 状态，无需遍历所有 `FD`，因此性能不会随监听的 `FD` 数量增多而下降。



#### epoll 中的 ET 和 LT

当FD有数据可读时，我们调用 epoll_wait（或者select、poll）可以得到通知。但是事件通知的模式有两种：

* **LevelTriggered**：简称 `LT`，也叫做水平触发。只要某个FD中有数据可读，每次调用epoll_wait都会得到通知。
* **EdgeTriggered**：简称 `ET`，也叫做边沿触发。只有在某个FD有状态变化时，调用epoll_wait才会被通知。

举个例子：

1. 假设一个客户端 `socket` 对应的 `FD` 已经注册到了`epoll` 实例中

2. 客户端socket发送了2kb的数据
3. 服务端调用epoll_wait，得到通知说FD就绪
4. 服务端从FD读取了1kb数据
5. 回到步骤3（再次调用epoll_wait，形成循环）



**结果：**

如果我们采用 LT 模式，因为 FD 中仍有1kb数据，则第⑤步依然会返回结果，并且得到通知
如果我们采用ET模式，因为第 ③ 步已经消费了FD可读事件，第⑤步FD状态没有变化，因此epoll_wait不会返回，数据无法读取，客户端响应超时。



LT 的惊群问题，以及使用 ET 对应的解决方案；

ET：可以使用非阻塞的逻辑，把数据完全读取；或者手动添加；



#### web 服务流程

基于`epoll`模式的web服务的基本流程如图：

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240608092008187.png" alt="image-20240608092008187" style="zoom:50%;" />

**流程如下：**

服务器启动以后，服务端会去调用epoll_create，创建一个epoll实例，epoll实例中包含两个数据

1. 红黑树（为空）：rb_root 用来去记录需要被监听的FD

2. 链表（为空）：list_head，用来存放已经就绪的FD

创建好了之后，会去调用epoll_ctl函数，此函数会会将需要监听的数据添加到rb_root中去，并且对当前这些存在于红黑树的节点设置回调函数，当这些被监听的数据一旦准备完成，就会被调用，而调用的结果就是将红黑树的fd添加到list_head中去(但是此时并没有完成)

3. 当第二步完成后，就会调用epoll_wait函数，这个函数会去校验是否有数据准备完毕（因为数据一旦准备就绪，就会被回调函数添加到list_head中），在等待了一段时间后(可以进行配置)，如果等够了超时时间，则返回没有数据，如果有，则进一步判断当前是什么事件，如果是建立连接时间，则调用accept() 接受客户端socket，拿到建立连接的socket，然后建立起来连接，如果是其他事件，则把数据进行写出



### 信号驱动 IO

信号驱动 `IO` 是与内核建立 `SIGIO` 的信号关联并设置回调，当内核有 `FD` 就绪时，会发出 `SIGIO` 信号通知用户，期间用户应用可以执行其它业务，无需阻塞等待。

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240608092605276.png" alt="image-20240608092605276" style="zoom:45%;" />

**阶段一：**

* 用户进程调用 `sigaction`，注册信号处理函数；
* 内核返回成功，开始监听 `FD`；
* 用户进程不阻塞等待，可以执行其它业务；
* 当内核数据就绪后，回调用户进程的 `SIGIO` 处理函数；

**阶段二：**

* 收到 `SIGIO` 回调信号；
* 调用 `recvfrom`，读取；
* 内核将数据拷贝到用户空间；
* 用户进程处理数据；

当有大量 `IO` 操作时，信号较多，`SIGIO` 处理函数不能及时处理可能导致信号队列溢出，而且内核空间与用户空间的频繁信号交互性能也较低。



### 异步 IO

异步IO的整个过程都是非阻塞的，用户进程调用完异步 `API` 后就可以去做其它事情，内核等待数据就绪并拷贝到用户空间后才会递交信号，通知用户进程。

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240608092936312.png" alt="image-20240608092936312" style="zoom:50%;" />

**阶段一：**

- 用户进程调用 `aio_read`，创建信号回调函数；
- 内核等待数据就绪；
- 用户进程无需阻塞，可以做任何事情；

**阶段二：**

- 内核数据就绪；
- 内核数据拷贝到用户缓冲区；
- 拷贝完成，内核递交信号触发 `aio_read` 中的回调函数；
- 用户进程处理数据；

**可以看到，异步 IO 模型中，用户进程在两个阶段都是非阻塞状态。**



### 总结 - 同步和异步

IO 操作是同步还是异步，关键看数据在内核空间与用户空间的拷贝过程（数据读写的 IO 操作），也就是阶段二是同步还是异步：

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240608093215220.png" alt="image-20240608093215220" style="zoom:50%;" />

## Redis网络模型

**Redis 到底是单线程还是多线程？**

* 如果仅仅聊 `Redis` 的核心业务部分（命令处理），答案是 **单线程**；
* 如果是聊整个 `Redis`，那么答案就是 **多线程**；

在 `Redis` 版本迭代过程中，在两个重要的时间节点上引入了 **多线程** 的支持：

* Redis v4.0：引入多线程异步处理一些耗时较久的任务，例如异步删除命令 `unlink`；
* Redis v6.0：在核心网络模型中引入 多线程，进一步提高对于多核 CPU 的利用率

因此，对于 Redis 的核心网络模型，在Redis 6.0之前确实都是单线程。是利用 `epoll`（Linux系统）这样的 IO 多路复用技术在事件循环中不断处理客户端情况。



**为什么Redis要选择单线程？**

* 抛开持久化不谈，Redis 是纯内存操作，执行速度非常快，它的 **性能瓶颈是网络延迟而不是执行速度**，因此多线程并不会带来巨大的性能提升；
* 多线程会导致 **过多的上下文切换**，带来不必要的开销；
* 引入多线程会面临 **线程安全** 问题，必然要引入线程锁这样的安全手段，实现复杂度增高，而且性能也会大打折扣。



`Redis` 通过 `IO` 多路复用来提高网络性能，并且支持各种不同的多路复用实现，并且将这些实现进行封装， 提供了统一的高性能事件库 `API` 库 `AE`：

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240708114350815.png" alt="image-20240708114350815" style="zoom:35%;" />

基于不同系统环境，选择不同的 IO 方案，如下：

```c++
/* ae.c */
#ifdef HAVE_EVPORT
#include "ae_evport.c"
#else
    #ifdef HAVE_EPOLL
    #include "ae_epoll.c"
    #else
        #ifdef HAVE_KQUEUE
        #include "ae_kqueue.c"
        #else
        #include "ae_select.c"
        #endif
    #endif
#endif
```

**初始化与事件监听逻辑：**

<img src="../media/2024-06-07-redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/image-20240708124506399.png" alt="image-20240708124506399" style="zoom:50%;" />



```c++
// 数据读处理器
void acceptTcpHandler(...) {
    // ...
    // 接收socket连接，获取FD
    fd = accept(s, sa, len);
    // ...
    // 创建connection，关联fd
    connection *conn = connCreateSocket();
    conn.fd = fd;
    // ... 
    // 内部调用aeApiAddEvent(fd, READABLE)，
    // 监听socket的FD读事件，并绑定读处理器readQueryFromClient
    connSetReadHandler(conn, readQueryFromClient);
}
```



`Redis` 单线程网络模型的整个流程：



















































